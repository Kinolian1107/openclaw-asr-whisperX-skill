---
name: gfile-asr-whisperx
description: >
  Download audio/video files from Google Drive and transcribe using WhisperX
  (faster-whisper + wav2vec2 alignment + speaker diarization). GPU-accelerated.
  Triggers on keywords: è½‰é€å­—ç¨¿, è½‰æ–‡å­—, transcribe, transcript, èªéŸ³è½‰æ–‡å­—, ASR, å­—å¹•, subtitle.
metadata:
  openclaw:
    emoji: "ğŸ™ï¸"
    requires:
      bins: ["ffmpeg", "gdown", "python3"]
    os: ["linux"]
---

# Google Drive ASR â€” WhisperX Mode

Transcribe audio/video from Google Drive using **WhisperX** (faster-whisper + wav2vec2 alignment + speaker diarization).

## How It Works

1. **WhisperX** loads faster-whisper with built-in VAD (Silero) â€” no hallucinations
2. **Batch inference** for maximum GPU throughput
3. **wav2vec2 forced alignment** for word-level accurate timestamps
4. **Optional speaker diarization** (identifies who said what)

## Mode Check

**Before executing, read `/home/kino/.openclaw/workspace/asr_config.json`.**
- If `"mode": "whisperx"` â†’ proceed with this skill
- If `"mode": "speaches"` â†’ use the `gfile-asr-speaches` skill instead
- User can switch modes with `/asrmode`

## Trigger Conditions

Activate when user sends Google Drive link + mentions transcription keywords (è½‰é€å­—ç¨¿, è½‰æ–‡å­—, transcribe, ASR, etc.).

## Prerequisites

Python venv at `/home/kino/asr/.venv/` with whisperx + PyTorch (CUDA 12.8 for RTX 50 series):

```bash
/usr/bin/python3.12 -m venv /home/kino/asr/.venv
/home/kino/asr/.venv/bin/pip install "whisperx>=3.3.4" gdown silero-vad
```

This installs WhisperX with all dependencies including PyTorch, pyannote-audio, faster-whisper, etc.

## Workflow

**CRITICAL: Run ALL steps without stopping. Deliver results via Telegram when done.**

### Step 1: Download from Google Drive

```bash
gdown "https://drive.google.com/uc?id={FILE_ID}" -O /home/kino/asr/{filename}
```

### Step 2: Run WhisperX Transcription

```bash
/home/kino/asr/.venv/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} --lang zh --format srt
```

For speaker diarization (requires HF_TOKEN env var):

```bash
HF_TOKEN=hf_xxx /home/kino/asr/.venv/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} --lang zh --format srt --diarize
```

### Step 3: Report Results & Deliver via Telegram

1. Copy SRT to workspace:
   ```bash
   cp /home/kino/asr/{basename}.srt /home/kino/.openclaw/workspace/{basename}.srt
   ```

2. Send via Telegram `message` tool:
   ```
   action: send
   message: "è½‰å¯«å®Œæˆï¼{basename}.srtï¼ˆWhisperXï¼Œ{duration}sï¼‰"
   filePath: /home/kino/.openclaw/workspace/{basename}.srt
   ```

## Configuration

| Parameter | Default | Description |
|-----------|---------|-------------|
| `WHISPERX_MODEL` | `large-v3-turbo` | Model size |
| `WHISPERX_DEVICE` | `auto` | Device (cuda/cpu/auto) |
| `ASR_COMPUTE_TYPE` | `int8` | Compute type |
| `WHISPERX_BATCH_SIZE` | `16` | Batch size for inference |
| `HF_TOKEN` | (none) | HuggingFace token for diarization |
| `HF_HOME` | `/home/kino/ollama-models/huggingface-hub` | Model cache |

## Supported Input

- **Audio**: MP3, WAV, M4A, FLAC, OGG, AAC, WMA
- **Video**: MP4, MKV, AVI, MOV, WebM, FLV
- **Sources**: Google Drive links, local file paths

## /asrmode Command

When user types `/asrmode`:

1. Read `/home/kino/.openclaw/workspace/asr_config.json`
2. Show current mode and options:
   ```
   ç›®å‰ ASR æ¨¡å¼ï¼šwhisperx
   
   å¯ç”¨æ¨¡å¼ï¼š
   1ï¸âƒ£ speaches â€” ffmpeg silencedetect + speaches Docker API (faster-whisper GPU)
   2ï¸âƒ£ whisperx â€” WhisperX æ‰¹æ¬¡æ¨ç† + wav2vec2 å°é½Š + èªªè©±è€…è¾¨è­˜
   
   è¼¸å…¥ 1 æˆ– 2 åˆ‡æ›æ¨¡å¼
   ```
3. After user selects, update `asr_config.json` `"mode"` field and confirm

## References

- [WhisperX](https://github.com/m-bain/whisperX) â€” INTERSPEECH 2023
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper)
- [Silero VAD](https://github.com/snakers4/silero-vad)
