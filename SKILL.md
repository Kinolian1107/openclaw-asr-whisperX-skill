---
name: gfile-asr-whisperx
description: >
  Download audio/video files from Google Drive and transcribe using WhisperX
  (faster-whisper + wav2vec2 alignment + speaker diarization). GPU-accelerated.
  Features: OpenCC ç¹é«”è¼¸å‡º, hotwords, corrections, speaker embedding matching.
  Triggers on keywords: è½‰é€å­—ç¨¿, è½‰æ–‡å­—, transcribe, transcript, èªéŸ³è½‰æ–‡å­—, ASR, å­—å¹•, subtitle,
  è¾¨è­˜æˆæ–‡å­—, èªéŸ³è¾¨è­˜.
metadata:
  openclaw:
    emoji: "ğŸ™ï¸"
    requires:
      bins: ["ffmpeg", "gdown", "python3"]
    os: ["linux"]
---

# Google Drive ASR â€” WhisperX Mode (v2)

Transcribe audio/video from Google Drive using **WhisperX** (faster-whisper + wav2vec2 alignment + speaker diarization).

## v2 Features

- **Topic-guided initial_prompt** â€” improves accuracy for domain-specific content
- **Audio denoising** â€” optional ffmpeg-based noise reduction
- **OpenCC s2twp** â€” auto-converts simplified â†’ traditional Chinese (Taiwan usage)
- **Hotwords** â€” faster-whisper native hotword boosting from `whisperx_hotwords.txt`
- **Corrections dictionary** â€” post-processing replacements from `asr_corrections.json`
- **Speaker embedding** â€” auto-extract speaker samples, match against registered DB
- **Speaker diarization** â€” pyannote speaker-diarization-3.1

## Mode Check

**Before executing, read `/home/kino/.openclaw/workspace/asr_config.json`.**
- If `"mode": "whisperx"` â†’ proceed with this skill
- If `"mode": "speaches"` â†’ use the `gfile-asr-speaches` skill instead
- User can switch modes with `/asrmode`

## Trigger Conditions

Activate when ANY of the following are true:

1. User sends a **Google Drive link** + mentions: è½‰é€å­—ç¨¿, è½‰æ–‡å­—, transcribe, transcript, èªéŸ³è½‰æ–‡å­—, ASR, å­—å¹•, subtitle, æ‘˜è¦, summary, åˆ†æ, è¾¨è­˜æˆæ–‡å­—, èªéŸ³è¾¨è­˜
2. User provides a **local file path** to audio/video and asks for transcription
3. User says "transcribe" or "è½‰é€å­—ç¨¿" referencing a previously downloaded file

## Pre-Transcription Interaction (IMPORTANT)

**Before starting transcription, check if the user has provided enough context:**

### 1. Topic / ä¸»é¡Œ
If the user did NOT mention the audio topic/subject, ask:
```
é€™å€‹éŸ³æª”çš„ä¸»é¡Œæ˜¯ä»€éº¼ï¼Ÿï¼ˆä¾‹å¦‚ï¼šè²¡ç¶“è¨è«–ã€æœƒè­°è¨˜éŒ„ã€èª²å ‚è¬›åº§ã€æ—¥å¸¸å°è©±ç­‰ï¼‰
æä¾›ä¸»é¡Œå¯ä»¥æå‡è¾¨è­˜æº–ç¢ºåº¦ ğŸ“ˆ
å¦‚æœä¸ç¢ºå®šï¼Œç›´æ¥èªªã€Œä¸ç”¨ã€æˆ‘å°±é–‹å§‹è½‰äº†ã€‚
```
Use the user's answer as `--topic` parameter.

### 2. Denoising / é™å™ª
If the user explicitly mentions é™å™ªã€é›œéŸ³å¤šã€éŸ³è³ªä¸å¥½ã€èƒŒæ™¯å™ªéŸ³, add `--denoise` flag.
**Do NOT proactively ask about denoising** â€” only apply when user mentions it.

### 3. Hotwords / ç†±è©
When the user says ã€Œå¢åŠ ç†±è©ã€ã€ã€ŒåŠ å…¥ç†±è©ã€ã€ã€Œæ–°å¢ hotwordã€ or similar:
- Append the new word(s) to `/home/kino/.openclaw/workspace/whisperx_hotwords.txt` (one per line)
- Confirm: "å·²æ–°å¢ç†±è©ï¼šXXX âœ… ä¸‹æ¬¡è½‰é€å­—ç¨¿æ™‚æœƒè‡ªå‹•ä½¿ç”¨ã€‚"

The script automatically loads hotwords from `whisperx_hotwords.txt` every run.
You can tell the user: "å¦‚æœæœ‰å°ˆæœ‰åè©æƒ³åŠ å¼·è¾¨è­˜ï¼Œå¯ä»¥è·Ÿæˆ‘èªªã€Œå¢åŠ ç†±è© XXXã€"

### 4. Speaker Diarization
If user asks to identify speakers (è¾¨è­˜èªªè©±è€…, åˆ†è¾¨è¬›è€…, diarize, èª°åœ¨èªªè©±), add `--diarize`.

## Prerequisites

Python venv at `/home/kino/asr/.venv-whisperx/` with whisperx + PyTorch nightly (CUDA 12.8 for RTX 50 series).

Required packages: `whisperx`, `gdown`, `opencc-python-reimplemented`, `soundfile`, `numpy`, `pyannote.audio`

## Workflow

**CRITICAL: Run ALL steps without stopping. Deliver results via Telegram when done.**

### Step 1: Download from Google Drive

```bash
gdown "https://drive.google.com/uc?id={FILE_ID}" -O /home/kino/asr/{filename}
```

### Step 2: Run WhisperX Transcription

Basic (with topic):
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} --lang zh --format srt --topic "ä¸»é¡Œæè¿°"
```

With denoising:
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} --lang zh --format srt --topic "ä¸»é¡Œ" --denoise
```

With speaker diarization:
```bash
HF_TOKEN=hf_xxx /home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} --lang zh --format srt --topic "ä¸»é¡Œ" --diarize
```

With subtitle splitting (limit characters per line):
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} --lang zh --format srt --topic "ä¸»é¡Œ" --max-chars 20
```

Full options:
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/transcribe_whisperx.py" \
    /home/kino/asr/{filename} \
    --lang zh \
    --format srt \
    --topic "ä¸»é¡Œæè¿°" \
    --denoise \
    --diarize \
    --max-chars 20 \
    --hotwords-file /home/kino/.openclaw/workspace/whisperx_hotwords.txt \
    --corrections-file /home/kino/.openclaw/workspace/asr_corrections.json
```

The script automatically:
- Loads hotwords from `whisperx_hotwords.txt` (boosts accuracy for domain terms)
- Loads corrections from `asr_corrections.json` (fixes known ASR errors)
- Converts output to traditional Chinese via OpenCC (s2twp mode)
- When `--diarize`: extracts speaker audio samples â†’ matches against speaker DB â†’ saves unknown speakers for future matching

### Step 3: Report Results & Deliver via Telegram

1. Copy SRT to workspace:
   ```bash
   cp /home/kino/asr/{basename}.srt /home/kino/.openclaw/workspace/{basename}.srt
   ```

2. Send via Telegram `message` tool:
   ```
   action: send
   message: "è½‰å¯«å®Œæˆï¼{basename}.srtï¼ˆWhisperXï¼Œ{duration}sï¼‰"
   filePath: /home/kino/.openclaw/workspace/{basename}.srt
   ```

3. If `--diarize` was used and there are unmatched speakers, inform the user:
   ```
   è¾¨è­˜å‡º {n} ä½èªªè©±è€…ã€‚
   æœªåŒ¹é…çš„èªªè©±è€…éŸ³æª”å·²ä¿å­˜åœ¨ï¼š{speaker_samples_dir}
   ä½ å¯ä»¥ä¹‹å¾Œå‘Šè¨´æˆ‘ã€ŒæŠŠ SPEAKER_00 å‘½åç‚º XXXã€ä¾†è¨»å†Šè²ç´‹ã€‚
   ```

## Speaker Embedding Management

### Registering a speaker (user uploads audio + provides name)

When user says "è¨»å†Šèªªè©±è€…"ã€"ä¸Šå‚³è²ç´‹"ã€"register speaker" etc:
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/speaker_embed.py" \
    register --name "åå­—" --audio /path/to/audio.wav
```

### Renaming a SPEAKER_XX from a previous session

When user says "æŠŠ SPEAKER_00 å‘½åç‚º XXX" or "rename speaker":
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/speaker_embed.py" \
    rename --sample-dir /home/kino/asr/speaker_samples/{session_dir} \
    --speaker SPEAKER_00 --name "åå­—"
```

### Listing registered speakers
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/speaker_embed.py" list
```

### Deleting a registered speaker
```bash
/home/kino/asr/.venv-whisperx/bin/python3 "${SKILL_DIR}/scripts/speaker_embed.py" \
    delete --name "åå­—"
```

## Configuration

| Parameter | Default | Description |
|-----------|---------|-------------|
| `WHISPERX_MODEL` | `large-v3-turbo` | Model size |
| `WHISPERX_DEVICE` | `auto` | Device (cuda/cpu/auto) |
| `ASR_COMPUTE_TYPE` | `int8` | Compute type |
| `WHISPERX_BATCH_SIZE` | `16` | Batch size for inference |
| `HF_TOKEN` | (none) | HuggingFace token for diarization |
| `HF_HOME` | `/home/kino/ollama-models/huggingface-hub` | Model cache |
| `--topic` | (none) | Topic description for initial_prompt |
| `--denoise` | false | Apply audio denoising |
| `--no-opencc` | false | Disable OpenCC traditional Chinese conversion |
| `--max-chars` | 0 (disabled) | Max characters per subtitle segment (recommended: 20 for Chinese) |

## Config Files

| File | Location | Purpose |
|------|----------|---------|
| `asr_config.json` | `/home/kino/.openclaw/workspace/` | ASR mode & settings |
| `whisperx_hotwords.txt` | `/home/kino/.openclaw/workspace/` | Hotword list (one per line) |
| `asr_corrections.json` | `/home/kino/.openclaw/workspace/` | Errorâ†’correct word mappings |
| `speakers.json` | `/home/kino/asr/speaker_embeddings/` | Registered speaker metadata |

## Supported Input

- **Audio**: MP3, WAV, M4A, FLAC, OGG, AAC, WMA
- **Video**: MP4, MKV, AVI, MOV, WebM, FLV
- **Sources**: Google Drive links, local file paths

## /asrmode Command

When user types `/asrmode`:

1. Read `/home/kino/.openclaw/workspace/asr_config.json`
2. Show current mode and options with inline buttons
3. After user selects, update `asr_config.json` `"mode"` field and confirm

## References

- [WhisperX](https://github.com/m-bain/whisperX) â€” INTERSPEECH 2023
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) â€” hotwords support in v1.2+
- [Silero VAD](https://github.com/snakers4/silero-vad)
- [OpenCC](https://github.com/BYVoid/OpenCC) â€” Chinese conversion
- [pyannote-audio](https://github.com/pyannote/pyannote-audio) â€” speaker diarization & embedding
